{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re\n",
        "import pandas as pd\n",
        "import random\n",
        "from string import ascii_lowercase\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "ZBSaJnzf4D2E"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK stop words\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to clean text: remove punctuation, stop words, and non-textual elements\n",
        "def clean_text(text):\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Split into words\n",
        "    words = text.split()\n",
        "    # Remove stop words and non-textual elements\n",
        "    cleaned_words = [word for word in words if word.lower() not in stop_words]\n",
        "    # strip underscores\n",
        "    cleaned_words = [word.strip('_') for word in cleaned_words]\n",
        "    # remove numbers\n",
        "    cleaned_words = [word for word in cleaned_words if not word.isnumeric()]\n",
        "    return ' '.join(cleaned_words)\n",
        "\n",
        "def find_start_end(text):\n",
        "    # Find the start and end of the main text\n",
        "    start_pattern = r\"\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK .+ \\*\\*\\*\"\n",
        "    end_pattern = r\"\\*\\*\\* END OF THIS PROJECT GUTENBERG EBOOK .+ \\*\\*\\*\"\n",
        "\n",
        "    start_match = re.search(start_pattern, text)\n",
        "    end_match = re.search(end_pattern, text)\n",
        "\n",
        "    start_idx = start_match.end() if start_match else 0\n",
        "    end_idx = end_match.start() if end_match else len(text)\n",
        "\n",
        "    return text[start_idx:end_idx]\n",
        "\n",
        "def process_book(url, label):\n",
        "    # Download the book text from the URL\n",
        "    response = requests.get(url)\n",
        "    response.encoding = 'utf-8'\n",
        "    text = response.text\n",
        "\n",
        "    # Extract the main text between start and end markers\n",
        "    main_text = find_start_end(text)\n",
        "\n",
        "    # Clean the main text\n",
        "    cleaned_text = clean_text(main_text)\n",
        "\n",
        "    # Extract words from the cleaned text\n",
        "    words = cleaned_text.split()\n",
        "\n",
        "    # Split words into partitions of 100 and take 200 random partitions\n",
        "    partitions = [words[i:i + 100] for i in range(0, len(words), 100)]\n",
        "    random_partitions = random.sample(partitions, min(200, len(partitions)))\n",
        "\n",
        "    return [(label, ' '.join(partition)) for partition in random_partitions]\n",
        "\n",
        "# Updated list of Gutenberg book URLs (Same as before, no change needed here)\n",
        "book_urls = [\n",
        "    'https://www.gutenberg.org/files/28054/28054-0.txt',  # The Brothers Karamazov\n",
        "    'https://www.gutenberg.org/files/1998/1998-0.txt',    # Thus Spoke Zarathustra\n",
        "    'https://www.gutenberg.org/files/2554/2554-0.txt',    # Crime and Punishment\n",
        "    'https://www.gutenberg.org/files/7849/7849-0.txt',    # The Trial\n",
        "    'https://www.gutenberg.org/files/600/600-0.txt',      # Notes from the Underground\n",
        "    'https://www.gutenberg.org/files/205/205-0.txt'       # Walden\n",
        "]\n",
        "\n",
        "book_authors = [\"Fyodor Dostoevsky\", \"Friedrich Nietzsche\", \"Fyodor Dostoevsky\", \"Franz Kafka\",  \"Fyodor Dostoevsky\", \"Henry David Thoreau\"]\n",
        "\n",
        "\n",
        "# Process all books\n",
        "all_partitions = []\n",
        "\n",
        "for url, label in zip(book_urls, book_authors):\n",
        "    book_partitions = process_book(url, label)\n",
        "    all_partitions.extend(book_partitions)\n",
        "\n",
        "# Convert to DataFrame\n",
        "partition_df = pd.DataFrame(all_partitions, columns=['Label', 'Words'])\n",
        "\n",
        "# Serialize DataFrame to CSV\n",
        "partition_df.to_csv('book_partitions_cleaned.csv', index=False)\n"
      ],
      "metadata": {
        "id": "RNTa27hDZczm",
        "outputId": "4500df3c-5709-428e-8e8f-24c8809bcf8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize the TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()"
      ],
      "metadata": {
        "id": "wy37iDXNr60x"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        ")\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.multiclass import OneVsOneClassifier"
      ],
      "metadata": {
        "id": "Jr5aTIB3AioG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_objs = [\n",
        "    OneVsRestClassifier(SVC(kernel=\"linear\")),\n",
        "    OneVsRestClassifier(SVC(kernel=\"rbf\")),\n",
        "    OneVsRestClassifier(SVC(kernel=\"poly\")),\n",
        "    RandomForestClassifier(),\n",
        "    GaussianNB(),\n",
        "    KNeighborsClassifier(),\n",
        "    OneVsOneClassifier(SGDClassifier()),\n",
        "    DecisionTreeClassifier(),\n",
        "    AdaBoostClassifier(),\n",
        "    XGBClassifier(random_state=69)\n",
        "    ]\n",
        "\n",
        "\n",
        "model_names = [\n",
        "    \"Linear SVC\",\n",
        "    \"Gaussian SVC\",\n",
        "    \"Polynomial SVC\",\n",
        "    \"RandomForestClassifier\",\n",
        "    \"Naive Bayes\",\n",
        "    \"KNeighborsClassifier\",\n",
        "    \"SGDClassifier\",\n",
        "    \"DecisionTreeClassifier\",\n",
        "    \"AdaBoostClassifier\",\n",
        "    \"XGBClassifier\",\n",
        "]"
      ],
      "metadata": {
        "id": "6sCOQgBa5k_Z"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=69)\n",
        "onehot_encoder = OneHotEncoder(sparse=False, drop='first')\n",
        "\n",
        "for m_obj, m_name in zip(model_objs, model_names):\n",
        "  acc_sum = 0\n",
        "  precision_sum = 0\n",
        "  recall_sum = 0\n",
        "  f1_sum = 0\n",
        "  roc_auc_sum = 0\n",
        "  for train_index,test_index in kf.split(partition_df[:600]):\n",
        "    train_data = partition_df.iloc[train_index]\n",
        "    test_data = partition_df.iloc[test_index]\n",
        "    X_train = train_data.drop(['Label'], axis=1)\n",
        "    y_train = train_data['Label']\n",
        "    X_test = test_data.drop(['Label'], axis=1)\n",
        "    y_test = test_data['Label']\n",
        "\n",
        "    # One-hot encoding for the label\n",
        "    y_train_onehot = onehot_encoder.fit_transform(train_data[['Label']])\n",
        "    y_test_onehot = onehot_encoder.transform(test_data[['Label']])\n",
        "\n",
        "    y_train_df = pd.DataFrame(y_train_onehot, columns=onehot_encoder.get_feature_names_out(['Label']))\n",
        "    y_test_df = pd.DataFrame(y_test_onehot, columns=onehot_encoder.get_feature_names_out(['Label']))\n",
        "\n",
        "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['Words'])\n",
        "    X_test_tfidf = tfidf_vectorizer.transform(X_test['Words'])\n",
        "    X_train_tfidf = pd.DataFrame(X_train_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "    X_test_tfidf = pd.DataFrame(X_test_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "    m_obj.fit(X_train_tfidf, y_train_df),\n",
        "    y_pred = m_obj.predict(X_test_tfidf)\n",
        "    acc = accuracy_score(y_pred, y_test_df)\n",
        "    precision = precision_score(y_pred, y_test_df, average='macro')\n",
        "    recall = recall_score(y_pred, y_test_df, average='macro')\n",
        "    f1 = f1_score(y_pred, y_test_df, average='macro')\n",
        "    roc_auc = roc_auc_score(y_test_df, y_pred, average='macro')\n",
        "\n",
        "    acc_sum+=acc\n",
        "    precision_sum += precision\n",
        "    recall_sum += recall\n",
        "    f1_sum += f1\n",
        "    roc_auc_sum += roc_auc\n",
        "\n",
        "  print(\"MODEL: {}\".format(m_name))\n",
        "  print(\"Average Accuracy: {}\".format(acc_sum/10))\n",
        "  print(\"Average Precision: {}\".format(precision_sum/10))\n",
        "  print(\"Average Recall: {}\".format(recall_sum/10))\n",
        "  print(\"Average F1: {}\".format(f1_sum/10))\n",
        "  print(\"Average ROC AUC: {}\".format(roc_auc_sum/10))\n",
        "  print()"
      ],
      "metadata": {
        "id": "p9Sv_Ko5ClfC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dca98bc-44b5-4ff5-a5d1-adaf76d18213"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MODEL: Linear SVC\n",
            "Average Accuracy: 0.9816666666666667\n",
            "Average Precision: 0.9781842425933173\n",
            "Average Recall: 0.9807565258710322\n",
            "Average F1: 0.9791160645503163\n",
            "Average ROC AUC: 0.9781842425933173\n",
            "\n",
            "MODEL: Gaussian SVC\n",
            "Average Accuracy: 0.9716666666666667\n",
            "Average Precision: 0.96080553054393\n",
            "Average Recall: 0.9752759384616058\n",
            "Average F1: 0.9668597847396516\n",
            "Average ROC AUC: 0.96080553054393\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "avGfSR3YPIPV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}