{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EBFoo5EZQSyV"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import re\n",
        "import pandas as pd\n",
        "import random\n",
        "from string import ascii_lowercase\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "7XC-gTERQouW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK stop words\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to clean text: remove punctuation, stop words, and non-textual elements\n",
        "def clean_text(text):\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Split into words\n",
        "    words = text.split()\n",
        "    # Remove stop words and non-textual elements\n",
        "    cleaned_words = [word for word in words if word.lower() not in stop_words]\n",
        "    # strip underscores\n",
        "    cleaned_words = [word.strip('_') for word in cleaned_words]\n",
        "    # remove numbers\n",
        "    cleaned_words = [word for word in cleaned_words if not word.isnumeric()]\n",
        "    # remove words that start with numbers\n",
        "    cleaned_words = [word for word in cleaned_words if not word[0].isnumeric()]\n",
        "    return ' '.join(cleaned_words)\n",
        "\n",
        "def find_start_end(text):\n",
        "    # Find the start and end of the main text\n",
        "    start_pattern = r\"\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK .+ \\*\\*\\*\"\n",
        "    end_pattern = r\"\\*\\*\\* END OF THIS PROJECT GUTENBERG EBOOK .+ \\*\\*\\*\"\n",
        "\n",
        "    start_match = re.search(start_pattern, text)\n",
        "    end_match = re.search(end_pattern, text)\n",
        "\n",
        "    start_idx = start_match.end() if start_match else 0\n",
        "    end_idx = end_match.start() if end_match else len(text)\n",
        "\n",
        "    return text[start_idx:end_idx]\n",
        "\n",
        "def process_book(url, label):\n",
        "    # Download the book text from the URL\n",
        "    response = requests.get(url)\n",
        "    response.encoding = 'utf-8'\n",
        "    text = response.text\n",
        "\n",
        "    # Extract the main text between start and end markers\n",
        "    main_text = find_start_end(text)\n",
        "\n",
        "    # Clean the main text\n",
        "    cleaned_text = clean_text(main_text)\n",
        "\n",
        "    # Extract words from the cleaned text\n",
        "    words = cleaned_text.split()\n",
        "\n",
        "    # Split words into partitions of 100 and take 200 random partitions\n",
        "    partitions = [words[i:i + 100] for i in range(0, len(words), 100)]\n",
        "    random_partitions = random.sample(partitions, min(200, len(partitions)))\n",
        "\n",
        "    return [(label, ' '.join(partition)) for partition in random_partitions]\n",
        "\n",
        "# Updated list of Gutenberg book URLs (Same as before, no change needed here)\n",
        "book_urls = [\n",
        "    'https://www.gutenberg.org/files/1342/1342-0.txt',  # Pride and Prejudice by Jane Austen\n",
        "    'https://www.gutenberg.org/files/768/768-0.txt',    # Wuthering Heights by Emily Brontë\n",
        "    'https://www.gutenberg.org/files/1260/1260-0.txt',  # Jane Eyre by Charlotte Brontë\n",
        "    'https://www.gutenberg.org/files/1400/1400-0.txt',  # Great Expectations by Charles Dickens\n",
        "    'https://www.gutenberg.org/files/145/145-0.txt',    # Middlemarch by George Eliot\n",
        "    'https://www.gutenberg.org/files/541/541.txt'       # The Age of Innocence by Edith Wharton\n",
        "]\n",
        "\n",
        "book_authors = [\"Jane Austen\", \"Emily Bronte\", \"Charlotte Bronte\", \"Charles Dickens\",  \"George Eliot\", \"Edith Wharton\"]\n",
        "\n",
        "\n",
        "# Process all books\n",
        "all_partitions = []\n",
        "\n",
        "for url, label in zip(book_urls, book_authors):\n",
        "    book_partitions = process_book(url, label)\n",
        "    all_partitions.extend(book_partitions)\n",
        "\n",
        "# Convert to DataFrame\n",
        "partition_df = pd.DataFrame(all_partitions, columns=['Label', 'Words']).sample(frac=1)\n",
        "\n",
        "# Serialize DataFrame to CSV\n",
        "partition_df.to_csv('book_partitions_cleaned.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05lwftILQl4Z",
        "outputId": "92d47ce9-64b5-4a6b-a4d3-f3ddd49a12fa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of Words"
      ],
      "metadata": {
        "id": "3BdkiMXMR-DS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "bow_vectorizer = CountVectorizer()"
      ],
      "metadata": {
        "id": "Xlj5GVDlR9x6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        ")\n",
        "from sklearn.multiclass import OneVsRestClassifier"
      ],
      "metadata": {
        "id": "AqiYHxTwRtp3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_objs = [\n",
        "    OneVsRestClassifier(SVC(kernel=\"rbf\")),\n",
        "    RandomForestClassifier(),\n",
        "    OneVsRestClassifier(GaussianNB()),\n",
        "    KNeighborsClassifier(),\n",
        "    OneVsRestClassifier(SGDClassifier()),\n",
        "    DecisionTreeClassifier(),\n",
        "    OneVsRestClassifier(AdaBoostClassifier()),\n",
        "    OneVsRestClassifier(XGBClassifier(random_state=69))\n",
        "    ]\n",
        "\n",
        "\n",
        "model_names = [\n",
        "    \"Gaussian SVC\",\n",
        "    \"RandomForestClassifier\",\n",
        "    \"Naive Bayes\",\n",
        "    \"KNeighborsClassifier\",\n",
        "    \"SGDClassifier\",\n",
        "    \"DecisionTreeClassifier\",\n",
        "    \"AdaBoostClassifier\",\n",
        "    \"XGBClassifier\",\n",
        "]"
      ],
      "metadata": {
        "id": "VEPfgQ_BSsTe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=69)\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "for m_obj, m_name in zip(model_objs, model_names):\n",
        "  acc_sum = 0\n",
        "  precision_sum = 0\n",
        "  recall_sum = 0\n",
        "  f1_sum = 0\n",
        "  roc_auc_sum = 0\n",
        "  for train_index,test_index in kf.split(partition_df[:600]):\n",
        "    train_data = partition_df.iloc[train_index]\n",
        "    test_data = partition_df.iloc[test_index]\n",
        "    X_train = train_data.drop(['Label'], axis=1)\n",
        "    y_train = train_data['Label']\n",
        "    X_test = test_data.drop(['Label'], axis=1)\n",
        "    y_test = test_data['Label']\n",
        "    # One-hot encoding for the label\n",
        "    y_train_onehot = onehot_encoder.fit_transform(train_data[['Label']])\n",
        "    y_test_onehot = onehot_encoder.transform(test_data[['Label']])\n",
        "\n",
        "    y_train_df = pd.DataFrame(y_train_onehot, columns=onehot_encoder.get_feature_names_out(['Label']))\n",
        "    y_test_df = pd.DataFrame(y_test_onehot, columns=onehot_encoder.get_feature_names_out(['Label']))\n",
        "\n",
        "    X_train_bow = bow_vectorizer.fit_transform(X_train['Words'])\n",
        "    X_test_bow = bow_vectorizer.transform(X_test['Words'])\n",
        "    X_train_bow = pd.DataFrame(X_train_bow.toarray(), columns=bow_vectorizer.get_feature_names_out())\n",
        "    X_test_bow = pd.DataFrame(X_test_bow.toarray(), columns=bow_vectorizer.get_feature_names_out())\n",
        "\n",
        "    m_obj.fit(X_train_bow, y_train_df),\n",
        "    y_pred = m_obj.predict(X_test_bow)\n",
        "    acc = accuracy_score(y_pred, y_test_df)\n",
        "    precision = precision_score(y_pred, y_test_df, average='macro')\n",
        "    recall = recall_score(y_pred, y_test_df, average='macro')\n",
        "    f1 = f1_score(y_pred, y_test_df, average='macro')\n",
        "    roc_auc = roc_auc_score(y_test_df, y_pred, average='macro')\n",
        "\n",
        "    acc_sum+=acc\n",
        "    precision_sum += precision\n",
        "    recall_sum += recall\n",
        "    f1_sum += f1\n",
        "    roc_auc_sum += roc_auc\n",
        "\n",
        "  print(\"MODEL: {}\".format(m_name))\n",
        "  print(\"Average Accuracy: {}\".format(acc_sum/10))\n",
        "  print(\"Average Precision: {}\".format(precision_sum/10))\n",
        "  print(\"Average Recall: {}\".format(recall_sum/10))\n",
        "  print(\"Average F1: {}\".format(f1_sum/10))\n",
        "  print(\"Average ROC AUC: {}\".format(roc_auc_sum/10))\n",
        "  print()\n",
        "\n",
        "\n",
        "# Plotting the graph\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'ROC AUC']\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.subplots_adjust(bottom=0.3)\n",
        "\n",
        "for i, metric_list in enumerate([accuracy_list, precision_list, recall_list, f1_list, roc_auc_list]):\n",
        "    plt.plot(model_names, metric_list, label=metrics[i])\n",
        "\n",
        "\n",
        "plt.title('Model Performance Comparison')\n",
        "plt.xlabel('Models')\n",
        "\n",
        "# Rotate model names vertically\n",
        "plt.xticks(rotation=90, ha='center')\n",
        "\n",
        "plt.ylabel('Metric Value')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOJnVMoiS45W",
        "outputId": "2202e536-153c-42d5-8175-aacc2463e596"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MODEL: Gaussian SVC\n",
            "Average Accuracy: 0.4066666666666666\n",
            "Average Precision: 0.3911202454952455\n",
            "Average Recall: 0.8166666666666667\n",
            "Average F1: 0.5029425516029367\n",
            "Average ROC AUC: 0.6955601227476228\n",
            "\n",
            "MODEL: RandomForestClassifier\n",
            "Average Accuracy: 0.45333333333333325\n",
            "Average Precision: 0.4379909211159211\n",
            "Average Recall: 0.9166666666666667\n",
            "Average F1: 0.5656918687308712\n",
            "Average ROC AUC: 0.7189954605579605\n",
            "\n",
            "MODEL: Naive Bayes\n",
            "Average Accuracy: 0.28833333333333333\n",
            "Average Precision: 0.28225980038480036\n",
            "Average Recall: 0.8305555555555554\n",
            "Average F1: 0.3998623817702765\n",
            "Average ROC AUC: 0.6409487407721104\n",
            "\n",
            "MODEL: KNeighborsClassifier\n",
            "Average Accuracy: 0.545\n",
            "Average Precision: 0.5322489316239316\n",
            "Average Recall: 0.8258295329043411\n",
            "Average F1: 0.5678368777996274\n",
            "Average ROC AUC: 0.7454802348856148\n",
            "\n",
            "MODEL: SGDClassifier\n",
            "Average Accuracy: 0.7449999999999999\n",
            "Average Precision: 0.7523293835793836\n",
            "Average Recall: 0.9312614237614237\n",
            "Average F1: 0.8216665953896738\n",
            "Average ROC AUC: 0.8710208203196448\n",
            "\n",
            "MODEL: DecisionTreeClassifier\n",
            "Average Accuracy: 0.8083333333333333\n",
            "Average Precision: 0.7962529831279831\n",
            "Average Recall: 0.8035055113731584\n",
            "Average F1: 0.7906396629849973\n",
            "Average ROC AUC: 0.8791953912889708\n",
            "\n",
            "MODEL: AdaBoostClassifier\n",
            "Average Accuracy: 0.7316666666666667\n",
            "Average Precision: 0.7695236013986013\n",
            "Average Recall: 0.9101836126836126\n",
            "Average F1: 0.8226499314201613\n",
            "Average ROC AUC: 0.8774334784137195\n",
            "\n",
            "MODEL: XGBClassifier\n",
            "Average Accuracy: 0.7033333333333334\n",
            "Average Precision: 0.698174371924372\n",
            "Average Recall: 0.96746632996633\n",
            "Average F1: 0.7921808171666129\n",
            "Average ROC AUC: 0.847383375850988\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF IDF"
      ],
      "metadata": {
        "id": "tnxZbrPaThR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize the TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()"
      ],
      "metadata": {
        "id": "wy37iDXNr60x"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=69)\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "for m_obj, m_name in zip(model_objs, model_names):\n",
        "  acc_sum = 0\n",
        "  precision_sum = 0\n",
        "  recall_sum = 0\n",
        "  f1_sum = 0\n",
        "  roc_auc_sum = 0\n",
        "  for train_index,test_index in kf.split(partition_df[:600]):\n",
        "    train_data = partition_df.iloc[train_index]\n",
        "    test_data = partition_df.iloc[test_index]\n",
        "    X_train = train_data.drop(['Label'], axis=1)\n",
        "    y_train = train_data['Label']\n",
        "    X_test = test_data.drop(['Label'], axis=1)\n",
        "    y_test = test_data['Label']\n",
        "    # One-hot encoding for the label\n",
        "    y_train_onehot = onehot_encoder.fit_transform(train_data[['Label']])\n",
        "    y_test_onehot = onehot_encoder.transform(test_data[['Label']])\n",
        "\n",
        "    y_train_df = pd.DataFrame(y_train_onehot, columns=onehot_encoder.get_feature_names_out(['Label']))\n",
        "    y_test_df = pd.DataFrame(y_test_onehot, columns=onehot_encoder.get_feature_names_out(['Label']))\n",
        "\n",
        "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['Words'])\n",
        "    X_test_tfidf = tfidf_vectorizer.transform(X_test['Words'])\n",
        "    X_train_tfidf = pd.DataFrame(X_train_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "    X_test_tfidf = pd.DataFrame(X_test_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "    m_obj.fit(X_train_tfidf, y_train_df),\n",
        "    y_pred = m_obj.predict(X_test_tfidf)\n",
        "    acc = accuracy_score(y_pred, y_test_df)\n",
        "    precision = precision_score(y_pred, y_test_df, average='macro')\n",
        "    recall = recall_score(y_pred, y_test_df, average='macro')\n",
        "    f1 = f1_score(y_pred, y_test_df, average='macro')\n",
        "    roc_auc = roc_auc_score(y_test_df, y_pred, average='macro')\n",
        "\n",
        "    acc_sum+=acc\n",
        "    precision_sum += precision\n",
        "    recall_sum += recall\n",
        "    f1_sum += f1\n",
        "    roc_auc_sum += roc_auc\n",
        "\n",
        "  print(\"MODEL: {}\".format(m_name))\n",
        "  print(\"Average Accuracy: {}\".format(acc_sum/10))\n",
        "  print(\"Average Precision: {}\".format(precision_sum/10))\n",
        "  print(\"Average Recall: {}\".format(recall_sum/10))\n",
        "  print(\"Average F1: {}\".format(f1_sum/10))\n",
        "  print(\"Average ROC AUC: {}\".format(roc_auc_sum/10))\n",
        "  print()\n",
        "\n",
        "  # Plotting the graph\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'ROC AUC']\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.subplots_adjust(bottom=0.3)\n",
        "\n",
        "for i, metric_list in enumerate([accuracy_list, precision_list, recall_list, f1_list, roc_auc_list]):\n",
        "    plt.plot(model_names, metric_list, label=metrics[i])\n",
        "\n",
        "\n",
        "plt.title('Model Performance Comparison')\n",
        "plt.xlabel('Models')\n",
        "\n",
        "# Rotate model names vertically\n",
        "plt.xticks(rotation=90, ha='center')\n",
        "\n",
        "plt.ylabel('Metric Value')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p9Sv_Ko5ClfC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "45665374-221d-4419-d585-f5a793c8aacd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-42851d1a271c>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mm_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/multiclass.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0mindptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m                 \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_predict_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m                 \u001b[0mindptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/multiclass.py\u001b[0m in \u001b[0;36m_predict_binary\u001b[0;34m(estimator, X)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# probabilities of the positive class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    783\u001b[0m         \u001b[0mtransformation\u001b[0m \u001b[0mof\u001b[0m \u001b[0movo\u001b[0m \u001b[0mdecision\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m         \"\"\"\n\u001b[0;32m--> 785\u001b[0;31m         \u001b[0mdec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    786\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function_shape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"ovr\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_ovr_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mdec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0mdec_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m             \u001b[0mdec_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0;31m# In binary case, we need to flip the sign of coef, intercept and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_dense_decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    554\u001b[0m             \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"precomputed\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m         return libsvm.decision_function(\n\u001b[0m\u001b[1;32m    557\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word embedding"
      ],
      "metadata": {
        "id": "X8Ck1Dx5UXTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glove_file_path= 'glove.6B/glove.6B.50d.txt'\n",
        "\n",
        "# Function to load GloVe embeddings into a dictionary\n",
        "def load_glove_embeddings(file_path):\n",
        "    embeddings = {}\n",
        "    with open(file_path, encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "# Load GloVe embeddings into the dictionary\n",
        "glove_embeddings = load_glove_embeddings(glove_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "YD8i6A3DUWqY",
        "outputId": "fdef175a-96d9-4504-e1b4-a6c58db2e8a0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'glove.6B/glove.6B.50d.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-d5f10b854dc7>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Load GloVe embeddings into the dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mglove_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_glove_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-d5f10b854dc7>\u001b[0m in \u001b[0;36mload_glove_embeddings\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_glove_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.6B/glove.6B.50d.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Assuming you have defined 'model' and 'names' somewhere in your code\n",
        "\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=69)\n",
        "onehot_encoder = OneHotEncoder(sparse=False, drop='first')\n",
        "\n",
        "# Lists to store metric values for each model\n",
        "accuracy_list = []\n",
        "precision_list = []\n",
        "recall_list = []\n",
        "f1_list = []\n",
        "roc_auc_list = []\n",
        "\n",
        "for m_obj, m_name in zip(model_objs, model_names):\n",
        "    acc_sum = 0\n",
        "    precision_sum = 0\n",
        "    recall_sum = 0\n",
        "    f1_sum = 0\n",
        "    roc_auc_sum = 0\n",
        "\n",
        "    for train_index, test_index in kf.split(partition_df):\n",
        "        train_data = partition_df.iloc[train_index]\n",
        "        test_data = partition_df.iloc[test_index]\n",
        "        X_train = train_data['text']\n",
        "        y_train = train_data['label']\n",
        "        X_test = test_data['text']\n",
        "        y_test = train_data['label']\n",
        "\n",
        "        # Create document embeddings using GloVe\n",
        "        X_train_glove = np.array([np.mean([glove_embeddings.get(word, np.zeros(50)) for word in sentence.split()], axis=0) for sentence in X_train])\n",
        "        X_test_glove = np.array([np.mean([glove_embeddings.get(word, np.zeros(50)) for word in sentence.split()], axis=0) for sentence in X_test])\n",
        "\n",
        "        # One-hot encoding for the label\n",
        "        y_train_onehot = onehot_encoder.fit_transform(train_data[['label']])\n",
        "        y_test_onehot = onehot_encoder.transform(test_data[['label']])\n",
        "        y_train_df = pd.DataFrame(y_train_onehot, columns=onehot_encoder.get_feature_names_out(['label']))\n",
        "        y_test_df = pd.DataFrame(y_test_onehot, columns=onehot_encoder.get_feature_names_out(['label']))\n",
        "\n",
        "        #Model Training and Prediction\n",
        "        m_obj.fit(X_train_glove, y_train_df)\n",
        "        y_pred = m_obj.predict(X_test_glove)\n",
        "\n",
        "        # Compute Evaluation Metrics\n",
        "        acc = accuracy_score(y_pred, y_test_df)\n",
        "        precision = precision_score(y_pred, y_test_df, average='macro')\n",
        "        recall = recall_score(y_pred, y_test_df, average='macro')\n",
        "        f1 = f1_score(y_pred, y_test_df, average='macro')\n",
        "        roc_auc = roc_auc_score(y_test_df, y_pred, average='macro')\n",
        "\n",
        "        acc_sum += acc\n",
        "        precision_sum += precision\n",
        "        recall_sum += recall\n",
        "        f1_sum += f1\n",
        "        roc_auc_sum += roc_auc\n",
        "\n",
        "    # Calculate average metric values for the model\n",
        "    avg_acc = acc_sum / 10\n",
        "    avg_precision = precision_sum / 10\n",
        "    avg_recall = recall_sum / 10\n",
        "    avg_f1 = f1_sum / 10\n",
        "    avg_roc_auc = roc_auc_sum / 10\n",
        "\n",
        "    # Append the average metric values to the respective lists\n",
        "    accuracy_list.append(avg_acc)\n",
        "    precision_list.append(avg_precision)\n",
        "    recall_list.append(avg_recall)\n",
        "    f1_list.append(avg_f1)\n",
        "    roc_auc_list.append(avg_roc_auc)\n",
        "\n",
        "    print(\"MODEL: {}\".format(m_name))\n",
        "    print(\"Average Accuracy: {}\".format(avg_acc))\n",
        "    print(\"Average Precision: {}\".format(avg_precision))\n",
        "    print(\"Average Recall: {}\".format(avg_recall))\n",
        "    print(\"Average F1: {}\".format(avg_f1))\n",
        "    print(\"Average ROC AUC: {}\".format(avg_roc_auc))\n",
        "    print()\n",
        "\n",
        "# Plotting the graph\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'ROC AUC']\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.subplots_adjust(bottom=0.3)\n",
        "\n",
        "for i, metric_list in enumerate([accuracy_list, precision_list, recall_list, f1_list, roc_auc_list]):\n",
        "    plt.plot(model_names, metric_list, label=metrics[i])\n",
        "\n",
        "\n",
        "plt.title('Model Performance Comparison')\n",
        "plt.xlabel('Models')\n",
        "\n",
        "# Rotate model names vertically\n",
        "plt.xticks(rotation=90, ha='center')\n",
        "\n",
        "plt.ylabel('Metric Value')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "isMZusL5TxX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT"
      ],
      "metadata": {
        "id": "3QW9DHmQJU2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_text"
      ],
      "metadata": {
        "id": "QBUu9VzuJ1qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text"
      ],
      "metadata": {
        "id": "avGfSR3YPIPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1'\n",
        "tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'"
      ],
      "metadata": {
        "id": "8t0dCypmJex7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
      ],
      "metadata": {
        "id": "QRBCF9JwI8be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
      ],
      "metadata": {
        "id": "1O4ncrD6JJz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_test = ['this is such an amazing movie!']\n",
        "text_preprocessed = bert_preprocess_model(text_test)\n",
        "\n",
        "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
        "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
        "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
        "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
        "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
      ],
      "metadata": {
        "id": "awzRkcK6JWjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_results = bert_model(text_preprocessed)\n",
        "\n",
        "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
        "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
        "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
        "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
        "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
      ],
      "metadata": {
        "id": "NVrP2qRRJOwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_classifier_model():\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dense(128, activation='relu', name='hidden_1')(net)\n",
        "  net = tf.keras.layers.Dropout(0.3)(net)\n",
        "  net = tf.keras.layers.Dense(6, activation='softmax', name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ],
      "metadata": {
        "id": "L9leNQPjJS-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model = build_classifier_model()\n",
        "bert_raw_result = classifier_model(tf.constant(text_test))\n",
        "print(bert_raw_result)"
      ],
      "metadata": {
        "id": "k4eyTvI-JcsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.plot_model(classifier_model)"
      ],
      "metadata": {
        "id": "Bl2MmOwfMIwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "metrics = tf.metrics.CategoricalAccuracy()"
      ],
      "metadata": {
        "id": "iEtQxDK2MPob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = partition_df.drop(['Label'], axis=1)\n",
        "y = partition_df['Label']"
      ],
      "metadata": {
        "id": "NHJFuUkWNP3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import warnings\n",
        "import numpy as np\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "y_train_onehot = onehot_encoder.fit_transform(np.array(y_train).reshape(-1,1))\n",
        "y_test_onehot = onehot_encoder.transform(np.array(y_test).reshape(-1,1))\n",
        "\n",
        "y_train_df = pd.DataFrame(y_train_onehot, columns=onehot_encoder.get_feature_names_out(['Label']))\n",
        "y_test_df = pd.DataFrame(y_test_onehot, columns=onehot_encoder.get_feature_names_out(['Label']))"
      ],
      "metadata": {
        "id": "nT50RFWiNXoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_df"
      ],
      "metadata": {
        "id": "ChqYMFCcQD9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "optimizer = tf.keras.optimizers.Adam(0.0001)"
      ],
      "metadata": {
        "id": "EQCQlwweMjPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)"
      ],
      "metadata": {
        "id": "OBiomcltMmFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model.summary()"
      ],
      "metadata": {
        "id": "5uzdbEyTVNdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = classifier_model.fit(X_train,y_train_df, batch_size=128, epochs=epochs, validation_data=(X_test, y_test_df))"
      ],
      "metadata": {
        "id": "_3-KEz34Ny7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "kZ855V5NZNfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = to_categorical(np.argmax(classifier_model.predict(X_test), axis=-1))"
      ],
      "metadata": {
        "id": "Jq6xrMG_Oznt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "id": "4AxmU_-zYOx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = accuracy_score(y_pred, y_test_df)\n",
        "precision = precision_score(y_pred, y_test_onehot, average='macro')\n",
        "recall = recall_score(y_pred, y_test_onehot, average='macro')\n",
        "f1 = f1_score(y_pred, y_test_onehot, average='macro')\n",
        "roc_auc = roc_auc_score(y_test_onehot, y_pred, average='macro')"
      ],
      "metadata": {
        "id": "Iihke22-Xmgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Average Accuracy: {}\".format(acc))\n",
        "print(\"Average Precision: {}\".format(precision))\n",
        "print(\"Average Recall: {}\".format(recall))\n",
        "print(\"Average F1: {}\".format(f1))\n",
        "print(\"Average ROC AUC: {}\".format(roc_auc))"
      ],
      "metadata": {
        "id": "krTXYdBWXwvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model.save('BERT_classifier.h5')"
      ],
      "metadata": {
        "id": "ekY3AWGfZbjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Error analysis: BERT"
      ],
      "metadata": {
        "id": "TIcNp-j_Z2dF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CqS2cns2Z3m_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}